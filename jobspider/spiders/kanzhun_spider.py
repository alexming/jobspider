# -*- encoding=utf-8 -*-import jsonfrom scrapy import Spider, log, Request, FormRequestfrom scrapy.selector import Selectorfrom jobspider.items import *from jobspider.kanzhun_items import *from jobspider.scrapy_requests.start_requests import *from jobspider.utils.tools import *class KanZhunSpider(Spider):    name = 'kanzhun'    download_delay = 10    randomize_download_delay = True    def __init__(self, reqHeaders, baseUrl, category = None, *args, **kwargs):        super(KanZhunSpider, self).__init__(*args, **kwargs)        self.reqHeaders = reqHeaders        self.baseUrl = baseUrl        self.token = ''    @classmethod    def from_settings(cls, settings):        reqHeaders = settings.get('KANZHUN_REQUEST_HEADERS', '')        baseUrl = settings.get('KANZHUN_BASEURL', 'http://www.kanzhun.com')        return cls(reqHeaders, baseUrl)    @classmethod    def from_crawler(cls, crawler, *args, **kwargs):        settings = crawler.settings        cls.stats = crawler.stats        return cls.from_settings(settings)    def create_url(self, parameters):        if parameters is None:            return ''        if not parameters.startswith('/'):            return self.baseUrl + '/' + parameters        else:            return self.baseUrl + parameters    #入口函数    def start_requests(self):        #首先需要登录        if self.token == '':            formdata = {}            formdata['_remember'] = 'on'            formdata['account'] = '707167666@qq.com'            formdata['password'] = 'fw!@#123'            formdata['redirect'] = self.baseUrl            formdata['remember'] = 'true'            yield FormRequest(url = self.create_url('login.json'),                headers = dict({'X-Requested-With': 'XMLHttpRequest'}, **self.reqHeaders),                formdata = formdata,                dont_filter = True,                meta = {'use_proxy': True},                callback = self.parse_login)        #已登陆，直接抓取        else:            yield self.start_city_requests(1)    def start_city_requests(self, id):        return Request(url = self.create_url('xsa%dp1.html' % id),            meta = {'use_proxy': True, 'CityId': id},            dont_filter = True,            callback = self.parse_city_list)    #登录解析    def parse_login(self, response):        js = json.loads(response.body)        if js:            if js['rescode'] == 1:                self.token = js['token'] #zPuPbc0yDh2xsgs                log.msg(u'登录成功.', level = log.INFO)                yield self.start_city_requests(1)            else:                self.token = ''                log.msg(u'登录失败,原因:%s' % js['resmsg'])    #列表解析    def parse_city_list(self, response):        hxs = Selector(response)        items = hxs.xpath("//div[@class='search-list-co-brief']/a[@class='fright mt15']")        for item in items:            link = first_item(item.xpath("@href").extract())            if link != '':                yield Request(url = self.create_url(link),                    meta = {'use_proxy': True, 'CityId': response.meta['CityId']},                    dont_filter = True,                    callback = self.parse_salary)    #薪资解析    def parse_salary(self, response):        hxs = Selector(response)        items = hxs.xpath("//table[@id='salaryDescTable']/tr[@data-url]")        for item in items:            src_url = self.create_url(first_item(item.xpath('td/a/@href').extract()))            if alreadyExistsUrl(src_url):                continue            #去重判别            salary = Salary()            name = first_item(item.xpath('td/a/text()').extract())            if name.endswith(')'):                ix = name.rfind('(')                if ix == -1:                    salary['job_name'] = name                    salary['job_count'] = 0                else:                    salary['job_name'] = name[0: ix]                    salary['job_count'] = int(name[ix + 1: -2])            else:                salary['job_name'] = name                salary['job_count'] = 0            salary['average'] = first_item(item.xpath("td[@class='s-d-average']/text()").extract())            salary['average'] = salary['average'].replace('￥', '').replace(',', '')            salary['company_logo'] = first_item(hxs.xpath("//a[@ka='com-logo']/img/@src").extract())            salary['src_url'] = src_url            #            company_url = first_item(hxs.xpath("//a[@ka='com-logo']/@href").extract())            if company_url is not None:                salary['company_url'] = self.create_url(company_url)                start = company_url.find('gso')                end = company_url.find('.html')                salary['company_code'] = company_url[start: end]            else:                salary['company_url'] = ''                salary['company_code'] = ''            #            comment_url = first_item(hxs.xpath(".//*[@id='fixedCoNav']/a[@ka='com-blocker1-review']/@href").extract())            if comment_url is not None:                salary['comment_url'] = self.create_url(comment_url)            else:                salary['comment_url'] = ''            #            co_info = hxs.xpath("//div[@class='co_info']")            salary['company_name'] = first_item(co_info.xpath("p[@id='companyName']/@data-companyname").extract())            salary['praise_rate'] = first_item(co_info.xpath("div[@class='msgs']/strong/text()").extract())            #            id = first_item(item.xpath('@id').extract())            if id != '':                id += '_C'                ul = hxs.xpath("//table[@id='salaryDescTable']/tr[@id='%s']/td/div/ul" % id)                if ul:                    salary['high'] = first_item(ul.xpath("li[@class='s-d-low']/text()").extract())                    salary['low'] = first_item(ul.xpath("li[@class='s-d-high']/text()").extract())                    salary['mark'] = first_item(ul.xpath("li[@class='s-d-mark']/a/em/text()").extract())                    salary['high'] = salary['high'].replace('￥', '').replace(',', '').lstrip(' ')                    salary['low'] = salary['low'].replace('￥', '').replace(',', '').lstrip(' ')            yield salary            #            if salary['company_url'] != '' and not alreadyExistsUrl(salary['company_url']):                yield Request(url = salary['company_url'],                    meta = {'use_proxy': True, 'company_id': salary['company_code']},                    dont_filter = True,                    callback = self.parse_company)            if salary['comment_url'] != '' and not alreadyExistsUrl(salary['comment_url']):                yield Request(url = salary['comment_url'],                    meta = {'use_proxy': True, 'CompanyName': salary['company_name']},                    dont_filter = True,                    callback = self.parse_comment_list)        #下页处理        link = first_item(hxs.xpath("//div[@class='page_wrap']/div/a[@class='p_next']/@href").extract())        if link is not None:            yield Request(url = self.create_url(link),                meta = {'use_proxy': True, 'CityId': response.meta['CityId']},                dont_filter = True,                callback = self.parse_salary)        else:            site_id = response.meta['CityId']            if site_id < 472:               site_id += 1            yield self.start_city_requests(site_id)    #企业解析    def parse_company(self, response):        #加入企业信息        hxs = Selector(response)        cmp = Company()        cmp['SiteID'] = 99        cmp['company_id'] = response.meta['company_id']        cmp['Credibility'] = 0        cmp['Licensed'] = 0        cmp['Yan'] = 0        cmp['FangXin'] = 0        cmp['CompanyName'] = FmtCmpNameCharacter(first_item(hxs.xpath(".//*[@id='companyName']/text()").extract()))        cmp['CityName'] = first_item(hxs.xpath("//a[@ka='com-city']/text()").extract())        cmp['AreaCode'] = ''        cmp['Industry'] = first_item(hxs.xpath("//a[@ka='com-industry']/text()").extract())        cmp['CompanyType'] = first_item(hxs.xpath(u"//strong[text()='公司性质']/../span/text()").extract())        cmp['CompanyScale'] = first_item(hxs.xpath(u"//strong[text()='公司规模']/../span/text()").extract())        cmp['Relation'] = first_item(hxs.xpath("//strong[text()='CEO']/../span/text()").extract())        cmpDesc = first_item(hxs.xpath("//article[@class='co_intro']/p").extract())        cmp['CompanyDesc'] = FmtSQLCharater(cmpDesc)        cmp['PraiseRate'] = first_item(hxs.xpath("//li[@class='cgtl_l']/div/text()").extract())        cmp['CompanyAddress'] = ''        cmp['Mobile'] = ''        cmp['GisLongitude'] = '0'        cmp['GisLatitude'] = '0'        cmp['UserId'] = ''        cmp['UserName'] = ''        cmp['ProvinceName'] = ''        cmp['WorkArea1'] = ''        cmp['AreaCode1'] = ''        yield cmp    #评论列表解析    def parse_comment_list(self, response):        CompanyName = response.meta['CompanyName']        url = ''        for url in response.xpath(".//*[@id='reviewPage']/div/dl/dd/p[@class='c_s_result_text']/a/@href").extract():            if url != '':                url = self.create_url(url)                if not alreadyExistsUrl(url):                    yield Request(url = url,                        meta = {'use_proxy': True, 'CompanyName': CompanyName},                        dont_filter = True,                        callback = self.parse_comment)        #请求下一页        link = first_item(response.xpath(".//*[@id='reviewPage']/div/div/a[@class='p_next']/@href").extract())        if link is not None:            yield Request(url = self.create_url(link),                meta = {'use_proxy': True},                dont_filter = True,                callback = self.parse_comment_list)     #评论解析    def parse_comment(self, response):        com = Comment()        com['SiteID'] = 99        com['CompanyName'] = response.meta['CompanyName']        com['AreaCode'] = ''        review_title = response.xpath(".//div[@class='wrap_style  mb15']/dl[@class='c_s_result']/dd[@class='c_s_result_detail']")        com['Title'] = first_item(review_title.xpath("p[@class='f_12 grey_99 view_address']//text()").extract())        com['Title'] = com['Title'].replace(u'\xa0', u' ')        com['Time'] = first_item(review_title.xpath("div[@class='u_c_s clearfix']/time/text()").extract())        com['Time'] = com['Time'].replace("\n", '')        com['Type'] = first_item(review_title.xpath("div[@class='c_s_v_result mb20']/em/text()").extract())        content = ''        for item in review_title.xpath("p[starts-with(@class, 'c_s_result_text')]//text()").extract():            content += item        com['Content'] = content        com['SrcUrl'] = response.url        if com['Type'] == u'很不满意':            com['TotalScore'] = 1        elif com['Type'] == u'不满意':            com['TotalScore'] = 2        elif com['Type'] == u'一般':            com['TotalScore'] = 3        elif com['Type'] == u'满意':            com['TotalScore'] = 4        elif com['Type'] == u'很满意':            com['TotalScore'] = 5        else:            com['TotalScore'] = 3        com['Time'] = FmtKZTime(com['Time'])        yield com